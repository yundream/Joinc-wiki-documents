#title Lucene 최적화에 대한 이슈들

== 최적화 목표 ==
 1. 100만건 문서를 기준
 1. 2개의 Term에 대해서 최악의 조건이 주어졌을 때 - 즉 Term이 100만개 모두에서 발생되었을 때 - 0.5 초 이내의 검색속도 보장.

== Lucene의 단점 ==
Lucene는 다음과 같은 단점을 가지고 있다. 
=== 너무 범용적이다 ===
많은 오픈 소스들이 그렇듯이 범용적으로 만들어졌다. 때문에 웹문서를 비롯해 다른 거의 대부분의 문서에 확장적용할 수 있으며, 범위검색, 날짜검색, 위치기반의 문장검색등의 다양한 검색을 적용할 수 있지만, 원하는 일만을 효율적으로 하기 위해서는 엔진자체가 지나치게 무거운 경우가 발생한다.

=== 위치기반 검색의 비 효율성 ===
두개 이상의 Term이 주어질 경우에 위치기반 검색을 수행하게 된다. 예를 들어 '''Linux Programing'''로 검색을 했을 경우 다음과 같이 거리가 가까울 수록 더 높은 score를 주게 된다.
 1. 저희 학원에서 '''Linux Programing'''에 대한 전문기술을 습득할 수 있습니다.
 1. 제가 이번에 '''Linux'' 환경을 새로 구축했습니다. C '''Programing'''을 위함인데요...
 1. '''Linux'''는 매우 좋은 운영체제다. Unix와 비슷한 철학을 가지고 있으며, 게다가 소스가 공개되어 있어서... 어쩌고.. 저쩌고... 해서 '''Programing''' 에 최적의 환경을..

이 경우 1번 2개의 Term의 위치가 가장 가까운 1번 문서에 가장 높은 점수를, 3번 문서에 가장 낮은 점수를 주게 된다. 

매우 당연하지만, 문제는 너무 보수적으로 위치기반 검색을 수행한다는 점으로, Term의 갯수만큼을 모두 돌면서 각 Term이 가진  Prox 파일을 검사해서 위치 연산을 한다는 점이다. 아래의 코드를 보면 Term이 발생하는 모든 위치에 대해서 Term의 갯수만큼 루프를 돌면서, mathLength를 구해서 위치연산을 하고 있음을 알 수 있다. 
{{{#!plain
final class SloppyPhraseScorer extends PhraseScorer {
	....
  do {
      PhrasePositions pp = (PhrasePositions) pq.pop();
      int start = pp.position;
      int next = ((PhrasePositions) pq.top()).position;
      for (int pos = start; pos <= next; pos = pp.position) {
         start = pos;                              // advance pp to min window
         if (!pp.nextPosition()) {
            done = true;                                  // ran out of a term -- done
            break;
         }
      }

      int matchLength = end - start;
      if (matchLength <= slop)
         freq += getSimilarity().sloppyFreq(matchLength); // score match

         if (pp.position > end)
            end = pp.position;
         pq.put(pp);                           // restore pq
  } while (!done);
  ...
}
}}}
이 방식은 위치검색과 관련된 정확한 정보를 제공해 주긴 하지만 대신 엄청나게 느릴 수 밖에 없다. 500만개의 문서셋에서 10개 정도의 Term에서 일치하는 모든 문서에 대해서 위의 연산이 이루어진다고 생각해 보라.


=== Score 연산 로직의 비효율성 ===
단일 Term에서는 관계가 없다. 2개 이상의 Term이 주어질 경우 각 Term에 대한 Score를 만들고 이를 did (문서 id)를 기준으로 merge sort를 해야 한다. 이때 did가 같다면, score를 더해주거나 하는 등의 연산을 해주어야 한다.

Lucene는 메모리 효율을 우선으로 한다. 버퍼의 크기를 32로 연산을 하는데, 이렇게 될경우 Term1과 Term2를 번갈아 가면서 merge sort를 해야 함으로 파일을 옮겨다니면서 seek를 하는 문제가 발생한다. prox 연산과 비교해서는 훨씬 작은 수치이긴 하지만, 시간을 줄일 필요가 있다. 

이 시간은 버퍼의 크기를 늘이거나 아니면, Term에 대한 모든 <did, score>값을 저장하기에 충분한 버퍼를 미리 할당해버리는 방법이 있을 것이다. 

=== Cache 모듈의 부재 ===
루신은 효율적인 캐쉬모델을 가지고 있지 않다. 여러번 검색되는 검색어에 대해서 <did, score>의 Top N만큼을 저장하는 캐쉬를 생각해 볼 수 있을 것이다. 혹은 DF의 크기가 일정 수 이상인 Term에 대해서만 캐쉬하는 방법도 생각해 볼 수 있다. 간단히 구현하길 원한다면 sqlite(:12)와 같은 DMBS를 이용하는 방법도 있을 것이다. 물론 고정 레코드 크기를 가지는 DB파일을 생성하는게 가장 빠르 겠지만, 그럭저럭 괜찮은 성능을 가진 프로토타입의 캐쉬에는 sqlite를 고려해볼만 하다. 

2000건의 <did, score>를 select 한다고 했을 때 걸리는 시간이 약 0.04초 정도이니 무리없이 사용가능할 것 같다. Java(:12)에서의 SQLite 정보는 [wiki:Site/SQLite SQLite 위키]를 참고하기 바란다.

=== C/C++로 재작성 ===
Java의 루신색인 검색 코드는 방대한 크기를 자랑한다. 그러나 웹문서 검색이든 도서 검색이든 범위를 한정시키면, 80% 정도는 필요가 없는 코드들이며, 매우 단순화 시킬 수 있다. 그렇다면 C/C++로 재작성할 수도 있는데, 이 경우 파일 탐색시간이라든지, 코드의 수행시간을 많이 줄일 수 있을 거라 생각된다. (Java의 코드 효율성이 떨어진다는 얘기가 아니다. Lucene자체가 확장/범용성을 우선시 해서 속도에는 최적화되어 있지 못하다)

개인적으로 C/C++로 재작성 하고 싶은데, 이유는 Java 언어보다는 C/C++에 익숙하기 때문이다. 물론 다른 언어로 작성된 Lucene 검색 엔진들도 존재한다. 
 * [http://sourceforge.net/projects/clucene c 언어]
 * [http://www.daniel-lemire.com/blog/archives/2005/04/04/lupy-python-lucene python 언어]
 * [http://search.cpan.org/~tmtm/Plucene-1.21/lib/Plucene.pm Perl 언어]

