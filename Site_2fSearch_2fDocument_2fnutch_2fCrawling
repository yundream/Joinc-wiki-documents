#title Nutch로 알아보는 Crawling 구조
[[UploadFile]]
[[TableOfContents]]
문서화를 먼저한 후 해당 프로그램을 이용해서 테스트를 진행할 것이기 때문에, 아래의 문서들은 충분히 검증되지 않았으며, 계속 수정될 것이다.
=== Nutch 소개 ===
Java로 된 open source 검색엔진인 nutch의 crawling 기능을 통해서 관련정보를 취득하기로 했다. 이 문서의 원본은 http://today.java.net/pub/a/today/2006/01/10/introduction-to-nutch-1.html 에서 찾을 수 있다. 

Nutch 는 Google 서취엔진을 대체하기 위해서 만들어졌으며, 다음과 같은 특징을 가지고 있다. 
 1. 투명성 : Nutch는 오픈소스다. 검색엔진의 가장 중요한 부분이기도한 랭킹 알고리즘과 그 구현이 완전히 공개되어 있다. 상용 검색엔진의 경우 ranking엔진과 관련된 부분은 완전히 감추어져 있다(랭킹과 관련된 기본적인 알고리즘이 공개되어 있긴 하지만). Nutch는 학습용 혹은 공공단체등에서 사용되는 정보의 중요도를 체크하기 위한 좋은 솔류션이다.
 1. 이해하기 쉬움 : Nutch는 검색엔진과 관련된 다양한 이론들을 포함하고 있다. 여기에는 distribute processing model로 사용되는 Map Reduce과 같은 것들을 포함하고 있다. Map Reduce는 Google 연구소에서 개발된 대량 Data Processing 엔진이다. 이외에도 Nutch는 최근에 연구되어지고 있는 검색알고리즘을 적용하고 테스트하기 위한 시도를 하고 있으며, 관련된 이론적인 지식을 알고 있다면, 쉽게 접근하고 이해할 수 있다.
 1. 확장성 : 다른 검색엔진들은 대부분 특화되어 있으며, 내부가 감추어져 있기 때문에 자신이 지원하는 환경이 아닌경우 확장이 어렵거나 불가능한 경우가 많다. 반면 Nutch는 일반적인 서취엔진을 구현하고 있으며, 소스가 공개되어 있기 때문에 쉽게 확장가능하다.

Nutch는 Local filesystem, intranet, Web등의 영역의 정보검색을 위해서 설치될 수 있다. 이들영역은 서로 다른 특징을 가지고 있는데, 지역 파일시스템을 예로 들자면, Web이나 intranet에서 필요한 caching copy가 불필요하다. 반면 Web의 경우 네트워크  에러라든지 네트워크 자원의 효율적 사용등을 위해서 복사본을 준비해야 한다. 게다가 수백만건 이상의 문서 데이터를 처리해야 하는 경우도 비일비재하며, 응답하지 않는 서버와 깨진링크, 연결/중복된 링크, 동일하게 복사된 문서들에 대해서 처리를 해야 한다.

=== Architecture ===
Nutch 는 crawler과 searcher 두부분으로 구성된다. crawler는 페이지를 수집하고, 페이지에 대한 index를 만들며, searcher은 유저의 요청을 받아서 필요한 정보를 찾아서 보여주는 일을 한다. index는 두개의 서로다른 구성요소간의 가교역할을 한다.   

==== Crawler ====
crawler은 crawl tool과 web database, segments와 인덱스등을 포함한 다양한 데이터 구조체를 만들고 유지하기 위한 툴들로 구성된 시스템이다. 이 시스템에 대해서 자세히 알아보도록 하겠다.

'''Web database (이하 WebDB)'''는 그래프 구성을 가지는 웹페이지의 정보들을 가지고 있는 특화된 데이터 구조를 지원하는 데이터베이스다. 웹페이지는 그래프 구조를 가지며 이러한 그래프는 수시로 재구성되므로, 일정 주기를 가지고 그래프를 재구성해서, 해당 그래프의 경로를 따러서 문서를 수집할 수 있도록 지원해야 한다. '''WebDB'''는 '''page'''와 '''링크''' 두개의 타입으로 구성된다. 
{{{#!plain
Web Page의 Grap 구조

               +-------+
        /------| Page  |-----\
       /       +-------+      \ Link
      /           |            \
 +-------+     +-------+    +-------+
 | Page  |-----| Page  |----| Page  |
 +-------+     +-------+    +-------+
      \           |           /
       \       +-------+     /
        \------| Page  |----/
               +-------+
}}}
웹 페이지들은 URL과 컨텐츠의 MD5해쉬정보를 이용해서 재구성된다. 이외에 각 페이지와 관계있는 정보들인 페이지에 포함된 링크의 갯수, 수집된 시간, 중요도(보통 해당 페이지를 참고하는 웹문서가 얼마나 많은지를 가지고 판단한다)등을 저장한다. 위의 Web Page의 연결구성을 보면 알겠지만, page가 node가 되고, Link정보로 연결되는 '''그래프''' 구조를 가진다. 

'''segment'''는 crawler에 의해서 수집되고 인덱스된 페이지의 모음이다. 이러한 segment로 부터 URL의 목록을 뽑아내어서 fetchlist를 만들어내고, 이러한 fetchlist를 이용해서 웹페이지를 브라우징하고 데이터를 가져오게 된다. segment는 디스크 공간을 차지하게 되므로, 무한정으로 segment를 유지하기에는 디스크 비용이 많이 소비되게 될것이다. 때문에 각각의 segment는 생성된 날짜와 시간정보를 가지고 있으며, 일정시간이 지난 후 에는 삭제가 된다.  

'''index'''는 가져온 모든 페이지를 색인화 한 것으로, 개개의 세그먼트의 색인들 병합해서 만들어진다. Nutch는 '''Lucence'''의 인덱싱 툴과 API를 이용하고 있다. 

==== crawltool ====
문서의 수집(crawling)는 다음과 같은 프로세스의 사이클을 가진다.
 1. 새로운 WebDB를 생성한다. 
 1. WebDB로 부터 수집이 최초로 시작될 root URL을 설정한다.    
 1. 새로운 segment의 WebDB로 부터 fetchlist를 생성한다. 
 1. fetchlist의 URL로부터 page를 수집한다.  
 1. 수집된 page로 부터 링크를 얻어오고, WebDB의 정보를 갱신한다.  
 1. 3-5단계를 계속 반복한다. 
 1. 중요도와 links정보를 Update한다.
 1. 수집한 페이지의 색인을 만든다.  
 1. 색인으로 부터 중복된 페이지를 제거한다.
 1. 효율적 검색을 위해서 단일 색인들을 병합한다. 

dedup 툴을 이용해서 segment 색인으로 부터 중복된 URL을 제거할 수 있다. 

=== Crawl의 작동 ===
그럼 Nutch를 Crawl로 작동시켜서, 사이트를 지정해서 정보를 수집해 보도록 하겠다. nutch는 Java(:12)환경에서 실행된다. 그러므로 Java환경을 만들고 환경변수 JAVA_HOME을 지정해줘야 한다. Java는 http://Java.sum.com 에서 다운받아서 설치하도록 한다.

이제 [http://lucene.apache.org/nutch/release download]페이지로 가서 nutch를 다운로드 받도록 한다. 필자는 0.7.1버젼을 가지고 테스트를 했다. 

attachment:nutch.gif

우선 테스트를 위해서 위의 구성을 가지는 간단한 웹페이지를 만들었다. 로컬시스템에 웹서버를 구성했으며, index.html, a.html, b.html, c.html의 4개의 페이지를 가지며, 위의 그림에서 처럼 링크를 가지도록 페이지의 내용을 채워넣었다. 

웹서버를 가동시켰으니 nutch의 설정을 변경해줘야 한다. 우선 root url을 지정해줘야 한다.  
{{{#!plain
# echo 'http://ubuntu/index.html' > urls
}}}
그다음 WebDB에 저장될 URL들의 필터를 지정해야 한다. 단지 필터를 통과한 URL만 WebDB에 저장이 된다. 수정해야될 파일은 conf/crawl-urlfilter.txt 이다. 필터는 정규표현(:12)을 지원한다. 여기에서는 http://ubuntu 사이트의 내용만을 가져올 것이므로
{{{#!plain
+^http://([a-z0-9]*\.)*MY.DOMAIN.NAME/
}}}
을
{{{#!plain
+^http://ubuntu/
}}}
로 수정한다.

이제 nutch를 이용해서 문서를 수집하면 된다.
{{{#!plain
# bin/nutch crawl urls -dir ubuntu.test -depth 3 >& crawl.log
}}}
'''crawl'''은 문서를 수집하겠다는 옵션이다. urls는 root url이 저장되어 있는 파일이다. '''-dir'''은 수집된 문서의 정보가 저장될 디렉토리이다. '''-depth''' 는 generate/fetch/update 주기의 범위를 결정하기 위해서 사용한다. 테스트 URL의 문서들은 단순하기 때문에 3단계면 충분할 것이다. 그러나 실제 사이트를 가지고 운용할려면 기본 5단계정도는 지정되어야 한다.  

=== Crawl 결과 분석 ===
문서는 수집되는 것으로 끝나는게 아니다. segments단위로 나누어야 하며, fetchlist, 색인, 문서내용등 연관된 정보를 데이터베이스화 해서 관리해야 한다. 때문에 몇개의 디렉토리로 구성된 독자적인 파일디비 형태로 저장된다. 다음은 ubuntu.test 디렉토리의 브라우징 결과다.

attachment:crawlsample.gif

파일시스템의 구조를 분석한다면, 수집된 문서를 어떻게 효율적인 구조로 인덱스하고, 구조화할 수 있는지에 대해서 알 수 있을것이다. 이에 대한 내용은 따로 다루기로 하겠다. 

=== WebDB ===
Crawl을 통해서 수집된문서는 DB화 된다. 여기에는 포함된링크, 페이지 갯수, DID(Document ID), 수집일, 문서의 중요도 등과 같은 중요한 정보들이 들어간다. 이들 정보는 업데이트 시켜야 하기 때문에 - 예를 들어, 해당 문서를 참고하는 웹페이지가 나왔다면, 문서의 중요도를 올려줘야 할것이다 - 정렬/검색/업데이트가 가능해야 하며, nutch는 이러한 도구를 제공한다. 우선 해당 사이트의 대략적인 정보를 알아보도록 하자. 
{{{#!plain
# bin/nutch readdb crawl-tinysite/db -stats
}}}
위의 명령을 실행하면 아래와 같은 정보를 볼 수 있을 것이다.
{{{#!plain
Stats for org.apache.nutch.db.WebDBReader@1c9b9ca
-------------------------------
Number of pages: 5
Number of links: 4
}}}
위의 정보를 통해서 ubuntu 사이트에서 5개의 페이지를 가져왔으며, 총 4개의 링크가 존재함을 확인할 수 있다.

이제 각페이지 단위로 상세정보를 알아보도록 하자.
{{{#!plain
# bin/nutch readdb ubuntu.test/db -dumppageurl
}}}
아래와 같은 결과를 확인할 수 있을 것이다.
{{{#!plain
Page 1: Version: 4
URL: http://ubuntu/
ID: f14dfdbdb13ff576277bbd58bf061d23
Next fetch: Wed Jul 26 17:51:36 KST 2006
Retries since fetch: 0
Retry interval: 30 days
Num outlinks: 1
Score: 1.0
NextScore: 1.0


Page 2: Version: 4
URL: http://ubuntu/a.html
ID: e1a0ed7a767bc0920b888c750224f39b
Next fetch: Wed Jul 26 17:51:38 KST 2006
Retries since fetch: 0
Retry interval: 30 days
Num outlinks: 3
Score: 1.0
NextScore: 1.0
}}}

다음과 같은 방법으로 각페이지의 링크상황을 확인할 수 있다.
{{{#!plain
# bin/nutch readdb ubuntu.test/db -dumplinks 
from http://ubuntu/a.html
 to http://ubuntu/b.html
 to http://ubuntu/c.html
 to http://ubuntu/index.html

from http://ubuntu/
 to http://ubuntu/a.html

from http://ubuntu/index.html
 to http://ubuntu/a.html
}}}

1
=== Segments ===
웹에는 엄청나게 많은 문서가 존재하며, 문서는 시시각각 변한다. 따라서 제대로 문서의 정보를 관리하기 위해서는 모든 문서의 복사본의 정보를 가지고 있어야 하겠지만, 이는 현실적으로 불가능하다. 때문에 이를 구획화해서 관리할 필요가 있다. nutch는 시간을 통해서 구획화한다. 예를들자면 오늘 하루동안 수집한 문서의 정보를 모아서 한달의 문서 통계를 만들고, 다시 한달의 문서통계를 모아서 일년의 문서정보를 유지하는 방식이다. 다음과 같은 방식을 통해서 segment정보를 확인할 수 있다. 
{{{#!plain
# bin/nutch segread -list -dir ubuntu.test/segments/
PARSED?   STARTED                 FINISHED                COUNT   DIR NAME
true      20060626-17:51:36       20060626-17:51:36       1       ubuntu.test/segments/20060626175136
true      20060626-17:51:38       20060626-17:51:38       1       ubuntu.test/segments/20060626175138
true      20060626-17:51:39       20060626-17:51:40       3       ubuntu.test/segments/20060626175139
TOTAL: 5 entries in 3 segments.
}}}

다음과 같은 방법으로 각 세그먼트의 상세정보를 얻을 수 있다.
{{{#!plain
s=`ls -d ubuntu.test/segments/* | head -1`
# bin/nutch segread -dump $s 
}}}

{{{#!plain
Recno:: 0
FetcherOutput::
FetchListEntry: version: 2
fetch: true
page: Version: 4
URL: http://ubuntu/
ID: 091f90073a9ba19470be1581e7adb865
Next fetch: Mon Jul 03 17:51:36 KST 2006
Retries since fetch: 0
Retry interval: 30 days
Num outlinks: 0
Score: 1.0
NextScore: 1.0

anchors: 0
Fetch Result:
MD5Hash: f14dfdbdb13ff576277bbd58bf061d23
ProtocolStatus: success(1), lastModified=0
FetchDate: Mon Jun 26 17:51:36 KST 2006

Content::
url: http://ubuntu/
base: http://ubuntu/
contentType: text/html
metadata: {Date=Mon, 26 Jun 2006 08:51:36 GMT, Server=Apache/2.2.0 (Unix) PHP/5.1.2, 
X-Powered-By=PHP/5.1.2, Connection=close, Content-Type=text/html, Content-Length=129}
Content:
<html>
<body>
Hello World <br>
1. <a href=a.html>Page A</a><br>
2. <a href=http://www.joinc.co.kr>Joinc</a><br>
</body>
</html>

ParseData::
Status: success(1,0)
Title:
Outlinks: 2
  outlink: toUrl: http://ubuntu/a.html anchor: Page A
  outlink: toUrl: http://www.joinc.co.kr/ anchor: Joinc
  Metadata: {Date=Mon, 26 Jun 2006 08:51:36 GMT, 
     CharEncodingForConversion=windows-1252, X-Powered-By=PHP/5.1.2, Server=Apache/2.2.0 (Unix) 
     PHP/5.1.2, Content-Type=text/html, Connection=close, Content-Length=129}

ParseText::
Hello World 1. Page A 2. Joinc
}}}
각 segments에 포함된 문서의 DID와 중요도, link, http header, 문서의 복사본등의 정보를 확인할 수 있다. 또한 HTML테그를 제거한 파싱된 문서의 확인도 가능하다. 이 파싱된 문서는 색인을 만들기 위한 정보로 사용된다. 

=== 색인 (index) ===
