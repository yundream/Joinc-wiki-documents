#title ZFS

[[TableOfContents]]
== ZFS ==
유닉스의 파일 시스템을 대체하기 위해서 SUN에서 개발한 파일 시스템으로 Solaris 10에 소개됐습니다. 단순한 파일 시스템으로 보기는 힘듭니다. LVM(:12)처럼 볼륨 메니저로 봐야 할 것 같은데요. 파일 시스템과 Logical Volume manager이 함께 구현돼 있습니다. 이 외에 snapshots, copy-on-write clones, continuous integrity checking, automatic repaire, RAID-Z 등 무슨 기능일지도 모를 것 같은 다양한 기능들을 가지고 있는 확장된 파일 시스템입니다. 

ZFS를 써본 사람들은 지상 최대의 파일 시스템이라는 찬사를 보내더라고요.

저는 Cloud 환경에서 File System을 선택하기 위한 테스트 목적으로 ZFS를 다뤄보기로 했습니다. 

ZFS의 기능을 좀 정리해 봤습니다. 각 기능들이 Cloud 환경 구성에 도움이 될만한자가 주요 관심사 입니다. 
  1. Data Integrigy
     다른 파일 시스템과 구분되는 가장 큰 특징입니다. 디스크 상의 유저 데이터를 보호해 주는 기능이죠. bit rot, cosmic radiation, current spikes, bugs in disk firmware, ghost writes 등으로 부터 데이터를 보호해 줍니다. 물론 Ext, XFS, JFS, ReiserFS, NTFS 등도 유저 데이터를 보호하기 위한 기능을 가지고 있기는 하지만, ZFS는 이들 보다 탁월한 성능을 보여줍니다. 
  1. Storage Pool
     LVM과 마찬가지로 하나 이상의 장치를 통합해서 관리할 수 있습니다. 이 가상의 Storage Pool을 '''zpools'''라고 합니다. zpool은 가상 장치로 block device 처럼 작동합니다. 블럭 장치들은 다양한 방식으로 조합할 수 있습니다. non-redundantly (RAID 0과 비슷), mirror ( RAID 1과 비슷 ), RAID-Z (RAID-5와 비슷), RAID-Z2 (RAID-6와 비슷) 등등입니다. RAID에 대해서 좀 알고 있어야 각 방식의 장점을 이해할 수 있을 것 같군요. 
  1. Capacity : ZFS는 128-bit 파일 시스템입니다. 이 정도면 거의 제한없이 사용할 수 있다고 볼 수 있지 싶습니다.
    * 2^48 개의 독립된 디렉토리를 다룰 수 있습니다.
    * 파일의 최대크기는 16 exabytes ( 16 X 10^18) 입니다.
    * zpool의 최대 크기는 : 256 zettabytes (2^78)
    * 시스템에서 zpools의 갯수 : 2^64
    * zpools에서 파일 시스템의 갯수 : 2^64

== Linux와 ZFS ==
Linux에 ZFS를 이식하기 위한 노력이 진행 중입니다. 이식은 두 가지 방향으로 이뤄지고 있습니다.
  1. native ZFS
    리눅스 커널이 ZFS를 지원하도록 하는 프로젝트
  1. zfs-fuse 
    fuse를 이용해서 ZFS를 지원하도록 하는 프로젝트
아직까지는 Linux에서 이용할 만한 수준이 아닌 것 같습니다. 지원하지 않는 기능들도 있는 것 같구요. zfs-fuse 같은 경우에는 성능에 문제가 좀 있습니다. 리눅스에서 ZFS를 경험하기 위한 목적으로 사용해야 할 것 같네요.

=== zfs-fuse ===
  1. apt-get install zfs-fuse
  1. zpoll create mypool /dev/sdb /dev/sdc /dev/sdd /dev/sde 
  1. zpool status mypool
  1. zfs create mypool/testzfs
  1. FS 성능 측정 : Bonnie++

== Solaris와 ZFS ==
=== opensolaris 설치 ===
솔라리스는 ZFS를 기본 파일 시스템으로 하고 있습니다. 생각난김에 x86기반의 opensolaris를 설치해서 경험해 보기로 했습니다. 우리에겐 VM이 있잖아요.
  * hypervisor : VirtualBox
    VirtualBox 와 opensolaris 모두 Oracle에서 개발하고 있으니, 궁합이 잘 맞을 거란 생각이 듭니다.
  * OpenSolaris 
    * [http://hub.opensolaris.org/bin/view/Main/downloads opensolaris 다운로드 사이트] 
    * 버전 : OpenSolaris 2009.06 (x86 LiveCD) 
인간적으로 설치는 윈도우보다 간단합니다. 클릭 몇 번이면 끝이니 설명은 건너 뛰겠습니다. 

솔라리스를 마지막으로 써본게 아마 9년전 쯤인것 같습니다. 2002년이던가 ? 투박한 CDE 화면을 생각했는데, 오 첫 화면이 멋지네요. GNOME 데스크탑입니다. 

{{{#!html
<table style="width:auto;"><tr><td><a href="https://picasaweb.google.com/lh/photo/hmHKyzX-cO58OIxvArlczg?feat=embedwebsite"><img src="https://lh4.googleusercontent.com/-GbCjddSyi8U/Tmh00qW5yZI/AAAAAAAABxM/38goFRk9-gM/s640/opensolaris.png" height="502" width="640" /></a></td></tr><tr><td style="font-family:arial,sans-serif; font-size:11px; text-align:right">보낸 사람 <a href="https://picasaweb.google.com/yundream/Linux?authuser=0&feat=embedwebsite">Linux</a></td></tr></table>
}}}

=== zpool ===
논리적 볼륨 관리자의 핵심은 장치를 아우르는 하나의 pool을 만드는 겁니다. LVM도 그렇고 ZFS도 마찬가지입니다. ZFS에서는 이 pool을 zpool이라고 부릅니다. 

테스트를 위해서 SATA 2G x 2 장치를 준비했습니다. 물론 가상 장치입니다. 리눅스에서 하던 것처럼 fdisk -l로 장치를 확인하려고 했더니, 사용방법이 다르군요. 그래서 format로 장치를 확인했습니다. 
{{{#!plain
# format
AVAILABLE DISK SELECTIONS:
       0. c7d0 <DEFAULT cyl 2085 alt 2 hd 255 sec 63>
          /pci@0,0/pci-ide@1,1/ide@0/cmdk@0,0
       1. c9t0d0 <ATA-VBOX HARDDISK-1.0-2.00GB>
          /pci@0,0/pci8086,2829@d/disk@0,0
       2. c9t1d0 <ATA-VBOX HARDDISK-1.0-2.00GB>
          /pci@0,0/pci8086,2829@d/disk@1,0
}}}

c9t0d0, c9t1d0을 zpool로 묶어야 겠습니다.
{{{#!plain
# zpool create tank c9t0d0 c9t1d0
}}}

제대로 만들어 졌는지 확인을 해보겠습니다.
{{{#!plain
# zpool list
NAME    SIZE   USED  AVAIL    CAP  HEALTH  ALTROOT
rpool  15.9G  3.80G  12.1G    23%  ONLINE  -
tank   3.97G   232K  3.97G     0%  ONLINE  -
}}}
tank라는 이름의 zpool이 만들어진걸 확인할 수 있습니다.

zfs로 파일 시스템 정보를 확인했습니다. 밑에 tank가 보이네요. 
{{{#!plain
# zfs list
NAME                         USED  AVAIL  REFER  MOUNTPOINT
rpool                       4.16G  11.5G  77.5K  /rpool
rpool/ROOT                  3.16G  11.5G    19K  legacy
rpool/ROOT/opensolaris      3.16G  11.5G  3.02G  /
rpool/dump                   511M  11.5G   511M  -
rpool/export                5.04M  11.5G    21K  /export
rpool/export/home           5.02M  11.5G    21K  /export/home
rpool/export/home/yundream     5M  11.5G     5M  /export/home/yundream
rpool/swap                   512M  11.8G   137M  -
tank                        74.5K  3.91G    19K  /tank
}}}

zfs는 디렉토리 형태로 파일 시스템을 관리합니다. tank 밑에 music, movie, source 3개의 파일 시스템을 만들어 보겠습니다.
{{{#!plain
# zfs create tank/music
# zfs create tank/movie
# zfs create tank/source
# zfs list
NAME                         USED  AVAIL  REFER  MOUNTPOINT
...
tank                         154K  3.91G    23K  /tank
tank/movie                    19K  3.91G    19K  /tank/movie
tank/music                    19K  3.91G    19K  /tank/music
tank/source                   19K  3.91G    19K  /tank/source
}}}
3개의 파일 시스템이 추가로 만들어지긴 했지만 volume을 모두 공유해서 사용하고 있습니다. 각 파일 시스템 별로 쿼터를 정해야 겠죠. /tank/movie를 1G를 사용하도록 쿼터를 할당해 보겠습니다. df로 확인까지 했습니다.
{{{#!plain
# zfs set quota=1g tank/movie
# zfs list | grep tank
tank                         154K  3.91G    23K  /tank
tank/movie                    19K  1024M    19K  /tank/movie
tank/music                    19K  3.91G    19K  /tank/music
tank/source                   19K  3.91G    19K  /tank/source

# df -h /tank/movie
Filesystem            Size  Used Avail Use% Mounted on
tank/movie            1.0G   19K  1.0G   1% /tank/movie
}}}
잘 할당 됐네요.

=== mirror, RAIDZ, RAIDZ2 ===
ZFS는 mirror와 RAIDZ, RAIDZ2를 지원합니다. 이들을 이해하려면 레이드레벨에 대한 지식이 필요합니다. 10G SATA Disk 6개를 추가해서 각 모드별로 테스트 하기로 했습니다.

RAIDZ과 RAIDZ2를 이해하기 위해서는 RAID Level에 대한 지식이 필요합니다. 간단히 정리해봤습니다. 스토리지에 전문적인 지식을 가지고 있지 않기 때문에 대략 조사했습니다. 언젠가 RAID에 대해서 자세히 조사해보고 싶네요.
  * RAID-0 : 데이터를 striped해서 저장합니다. 최소 두 개의 장치가 필요하겠죠. A, B 두개의 장치가 RAID-0으로 묶였다고 가정을 해보겠습니다. 여기에 1,2,3,4 4개의 데이터를 쓴다고 하면, 1을 A에 2를 B에 동시에 씁니다. 쓰기 성능을 높일 수 있겠죠. 읽기 성능도 높일 수 있는지는 잘 모르겠습니다.
  * RAID-1 : 디스크를 미러링 합니다. 최소 2개의 디스크가 필요하겠죠. 데이터를 중복 저장하기 때문에 하나의 디스크에 문제가 발생할 경우 완벽하게 복구할 수 있습니다. 두배의 저장공간이 필요하다는 단점이 있습니다. 
  * RAID-2 : 더 이상 사용하지 않습니다.
  * RAID-3 : parity를 사용하며, striping 합니다. striping을 위해서는 두 개 이상의 디스크가 필요하겠죠. 여기에 parity를 위한 디스크가 하나더 들어가서 최소 3개의 디스크가 필요한 구성입니다. dedicated parity사용
  * RAID-4 :  
  * RAID-5 : parity를 사용하며, striping 합니다.  
  * RAID-6 : 각 디스크에 분배되는 doule parity를 사용, striping. 두개의 드라이브까지 고장나는 것을 허용.
  * RAID-10: RAID 1 + RAID 0 입니다. mirror하고 striping 합니다. 


== Native ZFS on Linux ==
  * [wiki:Site/cloud/Storage/NativeZFSOnLinux Native ZFS on Linux] : 앞으로 오픈 솔라리스 대신 리눅스를 이용해서 zfs를 테스트할 생각입니다. 

== 관련 문서 ==
  * [wiki:Site/cloud/Storage/Nexentar Nexenta]
== History ==
  1. 작성일 : 2011년 9월 6일
  1. 수정이력
     1. 작성 : 2011/9/6 

[[Tag(Cloud,LVM,iSCSI)]]
