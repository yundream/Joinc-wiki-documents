head	1.15;
access;
symbols;
locks
	root:1.15; strict;
comment	@# @;


1.15
date	2014.11.13.05.48.35;	author root;	state Exp;
branches;
next	1.14;

1.14
date	2014.11.13.05.10.44;	author root;	state Exp;
branches;
next	1.13;

1.13
date	2014.11.13.03.40.50;	author root;	state Exp;
branches;
next	1.12;

1.12
date	2014.11.13.03.13.32;	author root;	state Exp;
branches;
next	1.11;

1.11
date	2014.11.13.03.10.22;	author root;	state Exp;
branches;
next	1.10;

1.10
date	2014.11.13.02.15.13;	author root;	state Exp;
branches;
next	1.9;

1.9
date	2014.11.13.02.14.28;	author root;	state Exp;
branches;
next	1.8;

1.8
date	2014.11.13.01.47.47;	author root;	state Exp;
branches;
next	1.7;

1.7
date	2014.11.12.05.55.18;	author root;	state Exp;
branches;
next	1.6;

1.6
date	2014.11.12.04.01.21;	author root;	state Exp;
branches;
next	1.5;

1.5
date	2014.11.12.03.51.20;	author root;	state Exp;
branches;
next	1.4;

1.4
date	2014.11.11.15.23.26;	author root;	state Exp;
branches;
next	1.3;

1.3
date	2014.11.11.14.37.50;	author root;	state Exp;
branches;
next	1.2;

1.2
date	2014.11.06.16.40.41;	author root;	state Exp;
branches;
next	1.1;

1.1
date	2014.11.06.06.30.53;	author root;	state Exp;
branches;
next	;


desc
@./data/text/man_2f12_2fMQTT_2fCluster
@


1.15
log
@1.214.223.250;;yundream;;
@
text
@#title MQTT Cluster 구성

[[TableOfContents]]
{{{#!html
<div class="row">
<div class="large-4 columns">
<div class="progress alert round"><span class="meter" style="60%">&nbsp;완성 60%</span></div>
</div>
</div>
}}}
== MQTT Cluster ==
MQTT Cluster를 구성하는 목적은 아래와 같다. 
  1. 대량의 메시지 처리 : 수백/수천만의 MQTT 클라이언트 요청을 처리할 수 있어야 한다. 당장 만들겠다는 것은 아니고, 목표는 그렇게 잡아보자는 이야기다. 꿈의 크기야 뭐 제한이 없으니까. 
  1. 고가용성 : 안정적으로 서비스 할 수 있어야 한다. 서버 구성요소 중 하나 이상에 문제가 생기더라도, 서비스 제공이 가능해야 한다.   
  1. 확장성 : Cluster에 노드를 추가하는 것으로 간단하게 서비스를 확장할 수 있어야 한다. 

=== 프로젝트 소개 ===
먼저 MQTT Cluster 시스템을 설계한다. 설계한 클러스터를 실제 환경에서 테스트할 수는 없겠지만, 프로토타이핑으로 기능적인 수준에서의 검증은 하려고 한다. 프로토타이핑 환경은 아래와 같이 구성한다. 
  1. 가상환경 : VirtualBox를 이용해서 가상환경을 만들어서 테스트 한다.  
  1. 설치 인프라 : 가상환경에서 테스트하지만, 운용은 AWS 인프라를 이용하는 걸 가정하고 설계한다. AWS의 ELB, Autoscaling기능을 이용하기 위함이다.
  1. 우분투 리눅스 : 테스트 운영체제는 우분투 리눅스 14.04로 한다. 
  1. MQTT 브로커 : Mosquitto MQTT 브로커를 사용한다.
  1. MQTT Proxy : 모스키토 앞단에 Proxy를 두고, 클라이언트 연결을 처리한다. 미들웨어로 작동한다. 
  1. REDIS : 데이터들의 저장소

그냥 만들면 재미 없으니까 서비스를 만들면서 '''MQTT Cluster'''를 구현하려고 한다. 서비스는 (가장 무난한) 멀티 클라이언트 메시징 시스템이다. 서비스의 기능은 다음과 같다.
  1. Private channel을 가진다. Private channel을 이용해서 단일 유저에게 메시지를 보낼 수 있다. 
  1. Group channel을 가진다. 한 명 이상의 유저가 참여하는 Topic이다. 뉴스라면 구독 서비스가 되겠고, 채팅이라면 채팅방 서비스가 되겠다. 
  1. 개인 메시지 함 : 유저가 연결하지 않은 경우, 유저 수신 메시지를 임시 저장하기 위한 서비스 

== 설계 ==
=== 시스템 구성 ===
시스템은 대략 다음과 같이 구성한다.

{{{#!html
<img src="https://docs.google.com/drawings/d/1T-EWLsSE80THwMQpTTlXnrxUe8mgqe8pEZ5M0b94-M0/pub?w=673&amp;h=411">
}}}

Node-1, Node-2는 유저의 Private channel(MQTT Topic)을 유지한다. Private channel로 보낸 메시지는 "Global message proc(이하 GMP)"로 보낸다. Global message proc는 이 메시지를 Message Queue에 적재한다. 이때, 유저 연결 테이블을 확인해서 유저가 어느 Node에 있는지를 찾아낸다. Node ID를 찾았다면, Node ID를 Key로 REDIS 에 밀어 넣는다. 

User ID를 Key로 하지 않는 이유는 User ID를 key로 할 경우, 각 노드는 User ID의 갯수만큼 list에 접근을 해야 하는 문제가 있기 때문이다. Node ID로 하면, 한번에 자신의 데이터를 읽을 수 있다. 

메시지 전송시점에  메시지 수신자가 연결하지 않았을 수도 있다. 이 경우 GMP는 유저 메시지 함에 메시지를 보내면 된다. 메시지 함은 User ID가 Key인 REDIS List로 관리한다. 새로 연결한 유저는 메시지 함에 읽지 않는 메시지가 있는지를 확인하는 과정을 거친다. GMP가 메시지함에 메시지를 쓰는 걸 끝내기 전에, 유저 가 연결을 끝내버릴 수도 있다. 이 경우, 유저는 다음 연결 전까지는 메시지함의 메시지를 확인할 수 없게 된다. 해결 방법을 고민해 보자면
  * 새로 연결한 유저는 수초 정도 후에, 메시지 함을 한번 읽도록 코드를 만든다. 
  * 메시지함을 읽는 초기 간격을 너무 길게 하면 메시지 반응성이 떨어지고, 너무 짧게하면, 메시지 함에 늦게 도착하는 경우가 생길 수 있다. 1초, 10초, 1분 이렇게 3번 정도 다른 간격으로 읽게하면 문제를 해결할 수 있다.

=== Group Table ===
라우팅 테이블의 데이터 구조는 대략 아래와 같을 것이다.
{{{#!plain
Group Table
  - Group A : {user-1, user-2, user-3}  
  - Group B : {user-1, user-4, user-5}
}}}

=== Message Queue ===
REDIS로 구축한다. Node ID를 Key로 한다. 

{{{#!html
<img src="https://docs.google.com/drawings/d/1s1WlTOtBOaas1UZyn1qLAh-ZRzt0HfhtUE5p1i2_C88/pub?w=491&amp;h=352">
}}}

=== 메시지 함 ===
유저가 받지 못한 메시지를 임시 저장하기 위한 공간이다. 유저가 연결하면 먼저 메시지함의 메시지를 처리하도록 한다. 역시 REDIS로 구성한다.

{{{#!html
<img src="https://docs.google.com/drawings/d/1km7_A5PI7xvSrIYVDpKtYMFQ5mBLnxKQBfZIE8vTdSM/pub?w=514&amp;h=350">
}}}

자료 타입으로 리스트(List)를 선택했다. User의 ID를 Key로 하고, 메시지는 '''LPUSH'''로 밀어 넣는다. 이렇게 구성하는 이유는 다음과 같다. 
  * User ID를 Key로 List를 만든다. 직관적이다.
  * 유저가 메시지를 소비하지 않을 경우 List에 계속 데이터가 쌓일 거다. 무한대로 쌓아둘 수는 없으니 크기에 제한을 둬야 하는데, LPUSH와 LTRIM 조합으로 하면, 리스트 크기를 관리하기가 쉽다. 예를들어 최근 1000개의 메시지만 남기고, 오래된 메시지는 삭제(혹은 다른 데이터베이스에 저장)하는 식의 구현이 가능하다. 

=== 메시지 흐름 ===
메시지 흐름을 자세히 그려보았다.

{{{#!html
<img src="https://docs.google.com/drawings/d/1S5xPtuMvE6h8Sg0U4XkCm5ki1PfD3OnXYqWGtvwPwJY/pub?w=933&amp;h=518">
}}}
  * MQTT Client는 '''Private topic'''만 가진다. 이 Private topic으로 모든 메시지를 받도록 구성한다. 예컨데, 그룹 메시지도 private topic "그룹타입의 메시지 형태"로 보낸다. 
  * 메시지는 Msg Sender로 보낸다.  
  * Msg Sender은 Global Message Proc로 보낸다.  
  * Global Message Proc는 private message인지, Group message인지를 확인해서 처리한다.
    * private message라면, user_id를 key로 하는 REDIS list에 밀어 넣는다.
    * group message라면, 유저 목록을 찾아서 user_id 갯수 만큼 REDIS list에 밀어 넣는다.
  * 각 Node에 있는 Message proc는 메시지를 읽어서 수신 유저의 topic에 PUB 한다. 

== 클러스터 관리 ==
스케일링을 위해서 새로운 노드가 클러스터에 추가될 수 있다. 관리상의 이유로 노드를 뺄 수도 있고, 문제가 생겨서 노드가 빠질 수도 있다. 클러스터 관리를 위해서 준비해야 할 것들을 고민해보려 한다. 

=== 오류 허용 ===
클러스터는 오류를 허용하는 시스템이다. 오류 허용 하지 않을려고 아둥바둥 하지 않겠다는 의미. 현재의 구성이 오류를 허용하는 구성인지를 살펴보자.

MQTT Broker가 뻗는 경우를 생각해 보자. 브로커가 그냥 시원하게 뻗어버리면, 모든 클라이언트의 연결이 끊길 테다. 그리고 이들은 LB에 의해서 다른 브로커 노드로 붙게 되니, 문제될게 없다. 유저가 연결이 끊겨있는 동안 수신한 메시지는 메시지 함에 쌓일테니 역시 문제 없다. 

REDIS 클러스터는 여기에서는 다루지 않는다.

== 구현 ==
=== 시스템 구성 ===
테스트가 목적이다. 따라서 최소한으로 구성한다. 

{{{#!html
<img src="https://docs.google.com/drawings/d/1wmeJ0ZQKN_-FVhzEZYxS3w4asfpxfuJMCcV8oJInM90/pub?w=587&amp;h=203">
}}}

=== MQTTT FrontEnd ===
MQTT Frontend는 3개 부분으로 구성된다.

{{{#!html
<img src="https://docs.google.com/drawings/d/1s9DkjeTCx_xHBIxBgrKUgqLzM87ixfssYWvNqte57No/pub?w=552&amp;h=212">
}}}
  * MQTT Proxy : 클라이언트 연결을 처리한다. 클라이언트가 연결하면, 연결 정보를 REDIS DB 에 기록한다.   
  * MQTT Broker : Mosquitto를 사용한다.
  * Message sender : MQTT Broker로 부터 메시지를 수신해서 '''GMP'''로 전송한다. MQTT를 IPC로 사용한다. 
  * Message Proc : REDIS 서버에서 메시지를 가져온다. 가져온 메시지는 유저 private channel로 분배한다. 

=== Mosquitto MQTT Broker 설치 === 
[wiki:man/12/MQTT/Tutorial MQTT Tutorial 참고]

=== REDIS 설치 ===
[wiki:man/12/REDIS/IntroDataType REDIS 설치와 기본 데이터타입들 참고]

=== MQTT Proxy ===
메시지는  Node ID를 Key로해서 각 노드가 가져가는 방식이다. 이를 위해서 GMP가 참조할 수 있는 {:User ID => Node ID} 테이블을 만들어야 한다. 유저 연결 요청을 받은 Proxy는 Node ID와 User ID를 '''REDIS''' 테이블에 저장한다. 

Proxy 코드다.

=== Message sender 개발 ===
Ruby로 개발한다. 이 프로그램은 '''/message''' 토픽에 대해서 sub를 요청한다.
@


1.14
log
@1.214.223.250;;yundream;;
@
text
@d111 18
@


1.13
log
@1.214.223.250;;yundream;;
@
text
@d13 1
a13 1
  1. 대량의 메시지 처리 : 수백/수천만의 MQTT 클라이언트 요청을 처리할 수 있어야 한다. 
d24 1
a24 1
  1. REDIS : 데이터들의 저장소  
d26 1
a26 1
서비스를 만들면서 '''MQTT Cluster'''를 구현하려고 한다. 서비스는 (가장 무난한) 멀티 클라이언트 메시징 시스템이다. 서비스의 기능은 다음과 같다.
@


1.12
log
@1.214.223.250;;yundream;;
@
text
@d58 4
@


1.11
log
@1.214.223.250;;yundream;;
@
text
@d20 1
a20 1
  1. 설치 인프라 : 가상환경에서 테스트하지만, 운용은 AWS 인프라를 이용한다. AWS의 ELB, Autoscaling기능을 이용하기 위함이다.
d24 1
d26 1
a26 2
서비스를 만들면서 '''MQTT Cluster'''를 구현하려고 한다. 서비스는 (가장 무난한) 멀티 클라이언트 메시징 시스템이다.
서비스의 기능은 다음과 같다.
d29 1
a29 5

=== Cluster 설계와 구현 ===
완성된 설계를 설명하는 아닌 "설계를 고민"하는게 목적이다. 아이디어가 나오는데로, 설계를 하고 이걸 개선시켜 나갈 것이다. 따라서 하나 이상의 설계가 있을 수 있으며, 중간에 수정되거나 아예 폐기되는 설계도 있을 것이다. 폐기되는 설계라고 하더라도 기록으로는 남길 것이다.  

가능성이 있는 설계의 경우 직접 구현한다.
@


1.10
log
@1.214.223.250;;yundream;;
@
text
@d43 7
a49 1
Node-1, Node-2는 유저의 Private channel(MQTT Topic)을 유지한다. Private channel로 보낸 메시지는 "Global message proc"로 보낸다. 이 메시지를 Message Queue에 적재한다. Private channel로 향하는 메시지라면, Private channel ID(User ID)를 Key로 해서 REDIS List에 밀어 넣기만 하면 된다. Group 메시지의 경우에는 Group db에서 Group에 포함된 유저 목록을 확인 한 후 각 User id에 대해서 메시지를 밀어 넣는다. 
d60 4
a63 1
REDIS로 구축하기로 했다.
d66 1
a66 1
<img src="https://docs.google.com/drawings/d/1km7_A5PI7xvSrIYVDpKtYMFQ5mBLnxKQBfZIE8vTdSM/pub?w=504&amp;h=216">
a72 2
REDIS Cluster 구성이슈도 있는데, 이 문서의 범위를 벗어난다. [wiki:man/12/REDIS REDIS 페이지]에서 (기회가 되면)자세히 다룰 계획이다.

d79 7
a85 4
  * MQTT Client는 '''Private topic'''만 연결하도록 한다. 이 Private topic으로 모든 메시지를 받도록 구성한다. 주제에 따라 topic을 분리할 수도 있겠는데, 복잡해 질 것 같다. 메시지 타입으로 구분한다.  
  * 메시지는 즉시 Message Router로 보낸다. Message Router은 메시지의 경로를 확인한다. 내부에 있다면, 목적지 Private topic으로 직접 전송하면 되겠다. 외부에 있다면, Global Message Router로 전송한다. 
  * GMR은 노드를 찾아서 메시지를 큐에 샇는다. REDIS로 과리한다.  
  * 라우팅 테이블은 REDIS로 관리한다.
d93 1
a93 16
MQTT Broker가 뻗는 경우를 생각해 보자. 브로커가 그냥 시원하게 뻗어버리면, 그러니까 깔끔하게 connection error 혹은 socket close 에러가 떨어버 버리면, 클라이언트는 연결을 끊고 다시 연결요청을 한다. 그러면 LB가 살아남은 브로커 중 한녀석에게 중계할테니 서비스는 여전히 잘 이용할 수 있다. 

Socket은 여전히 붙어 있는데, 문제가 생길 경우가 있다. MQTT PING, 메시지 모니터링, 응답 지연 시간 확인 등 다양한 방법으로  문제를 진단해야 한다(다루어야 할 내용이 많다. 이건 별도의 문서로). 진단이 되면, LB를 조작해서 (깔끔하게) 문제가 되는 노드의 연결을 끊어버린다. 클라이언트는 자연스럽게 다른 브로커로 분산되고, 개발자는 문제의 원인을 찾아서 해결하면 되겠다. 

노드가 내려갈 때, 아직 처리 중인 메시지를 어떻게 해야 하느냐란 이슈가 있다. 각 구간별로 살펴봐야 하는데, 서비스의 특징에 따라서 정책이 달라질 수도 있는 복잡한 문제다. 이 문제는 문서 말미에 (가능하다면) 따로 다루도록 하겠다. 

=== 단순화 ===
오류를 허용하는 시스템이라는 것은 노드의 추가와 삭제가 자유롭다는 것을 의미한다. 노드가 네트워크에 추가되면 자동으로 시스템에 넣어야 하고, 노드가 빠지면 시스템에서 빼서 메시지가 흐르지 않도록 해야 할거다. 

난 단순하게 관리할 생각이다. 노드를 빼는 것은  LB의 healthcheck를 이용한다. 노드가 빠질 경우, 대체 노드를 실행해야 할 건데 이건 Auto scaling에 맡긴다. 안정적인 서비스를 위한 최소 노드(2대는 유지해야 할 거다.)만 확보된다면, 노드를 즉시 실행해야 할 필요는 없다. 서비스 상황을 보면서 올리면 된다. 

GMR에서 MQTT Broker로 메시지를 보내는 경우에는 라우팅 테이블을 이용한다. 이 라우팅 테이블은 노드 상황에 따라서 동적으로 갱신을 해야 한다. 내가 생각하는 구성이다. 주키퍼(Zookeeper)를 이용하는 방법도 있겠는데, 이 프로젝트에 사용하기에는 지나치게 무겁고 복잡한 툴 같다. 단순한 기술을 사용할 수 있다면, 단순한 기술을 선택한다는게 내 방침이다.

{{{#!html
<img src="https://docs.google.com/drawings/d/1Mvi_INHBr5Kkbl2un5tzALKbGY_niHFuzk2dgqldAfU/pub?w=448&amp;h=254">
}}}
d95 1
a95 5
노드가 떨어져 나가면서, 자신의 상태를 능동적으로 데이터 베이스에 쓰면 좋겠지만 이미 노드에 문제가 생긴 상황에서 이를 장담할 수가 없다. 따라서 노드의 상태를 모니터링하는 외부 장치가 필요하다. 나는 ELB를 이용해서 노드의 상태를 모니터링 하기로 했다.
  1. ELB Monitering Node에서 주기적으로 ELB 상태를 모니터링 한다.    
  1. Node가 빠지면, ELB Monitring은 그 상황을 알 수 있다. 주기적으로 모니터링 하면 된다. 
  1. Routing Table Manager를 이용해서 Routing Table 정보를 갱신한다. Node의 Instance ID를 key로 전부 지워주면 된다. 
다 좋은데, Node의 상태 정보가 즉시 Routing table로 반영되지 않는 것 때문에 뒷통수가 근질근질 하다. ELB로 부터 요청은 못받는데, GMR로 부터는 메시지를 받을 수 있는 상황. ELB로 부터 요청은 받는데, GMR이 메시지를 보내지 못하는 상황. Node는 빠졌지만 아직 Routing 테이블이 갱신되지 않은 상황 등등 다양한 상황이 있을 수 있다. 이 문제는 문서의 범위를 넘어간다. 다른 문서에서 고민을 해봐야 겠다.
@


1.9
log
@1.214.223.250;;yundream;;
@
text
@d5 5
a9 1
<div class="progress alert round"><span class="meter" style="60%">완성 60%</span></div>
@


1.8
log
@1.214.223.250;;yundream;;
@
text
@d4 3
d31 1
a31 1
== 설계 - 1 ==
d39 1
a39 3
유저는 Node-1, Node-2에 접근해서 개인 채널(Private channel)을 만들 수 있다. 1:1 메시지와 그룹 메시지는 모두 개인 채널로 전달된다. 개인 채널과 유저는 1:1관계이므로 LB(Load balancer)로 분산할 수 있다. 이 구간에서는 MQTT를 사용한다.  

개인 채널로 메시지가 들어오면(from), 이 메시지가 누구에게(to) 향하는지를 검사 해서, 적당한 Node로 보내야 한다. To가 같은 노드에 없다면, "Messgage Router"로 메시지를 전송한다. 이 구간은 HTTP 프로토콜을 이용한다. 메시지 요청을 받은 Node는 라우팅 테이블에서 "To"가 위치한 노드를 찾은 후, 노드 이름을 Key로 Message Queue에 메시지를 전송한다. 각 노드들은 Message Queue에 있는 메시지를 읽어와서 처리한다.  
d41 1
a41 3
To가 개인일 경우에는 메시지를 한번 만 전송하면 되지만, 그룹으로 보내는 거라면 메시지 복사가 발생할 수 있다. 100명의 유저가 가입한 그룹이고, 마침 유저가 100개의 노드에 흩어져 있다면 최대 100번의 데이터복사가 발생할 테다. 효율적으로 처리하기 위한 방법을 고민해 보는 것도 재미있을 것 같다. 

=== Routing Table ===
d44 3
a46 7
Groups Table
  - Group_1 : [Node 1, Node 3, Node 4, Node 5 ....]
  - Group_2 : [Node 3, Node 5, Node 6 ....]
Connection Table
  - User_1 : Node 1 
  - User_2 : Node 2 
  - User_3 : Node 1
a47 3
Group 테이블은 개별 유저 채널을 알고 있을 필요는 없다. 한명이라도 유저가 있는 노드의 목록만 알고 있으면 된다. 노드에 두 명이상의 유저가 있을 수 있는데, 노드에 전달된 메시지를 유저 Topic에 Publish하는 것은 각 노드에서 처리하면 된다. 

Connection 테이블은 개별 유저의 노드 정보를 가지고 있다. 유저가 있는 노드로 메시지를 보내면, 노드가 목적지 Topic에 publish 한다.  
d70 2
a71 2
  * GMR은 노드를 찾아서 메시지를 큐에 샇는다. 
라우팅 테이블은 REDIS로 관리한다.
@


1.7
log
@1.214.223.250;;yundream;;
@
text
@d18 1
a18 1
서비스를 만들면서 '''MQTT Cluster'''를 구현하려고 한다. 서비스는 (가장 무난한) 멀티 대화방을 가진 채팅 시스템이다.
d20 2
a21 2
  1. Private channel을 가진다. Private channel을 이용해서 1:1 대화를 할 수 있다. 
  1. Group channel을 가진다. 대화방이다. 한명 이상의 유저가 참여해서 대화를 나눌 수 있다. 대화 내용은 대화방의 모든 유저에게 브로드캐스팅 된다.  
d56 14
a117 2
Message Queue는 MQTT로 만들기로 했다. RabbitMQ 같은 걸로 만들려니 귀찮더라. Message Queue는 GMR로 부터 전송 받은 메시지를 MQTT Broker의 IP주소를 이름으로 하는 토픽에 pub 하는 걸로 메시지를 전송한다.

@


1.6
log
@1.214.223.250;;yundream;;
@
text
@d97 1
a97 1
=== 구성 ===
d99 13
@


1.5
log
@1.214.223.250;;yundream;;
@
text
@d97 2
@


1.4
log
@119.64.102.68;;yundream;;
@
text
@d13 1
d22 1
d38 1
a38 1
개인 채널로 메시지가 들어오면(from), 이 메시지가 누구에게(to) 향하는지를 검사 해서, 적당한 Node로 보내야 한다. To가 같은 노드에 없다면, "Messgage Router"로 메시지를 전송한다. 이 구간은 HTTP 프로토콜을 이용한다. 메시지 요청을 받은 Node는 라우팅 테이블에서 "To"가 위치한 노드를 찾은 후, 해당 노드에 메시지를 전송한다. 
d40 1
a40 1
To가 개인일 경우에는 메시지를 한번 만 전송하면 되지만, 그룹으로 보내는 거라면 메시지 복사가 발생할 수 있다. 100명의 유저가 가입한 그룹이고, 마침 유저가 각 노드에 흩어져 있다면 최대 100번의 데이터복사가 발생할 테다. 실제 구성에서는 HTTP대신 다른 프로토콜을 사용해야 할 거다.(메시지큐는 좋은 안이 될 수 있다.) 
a55 1

d64 1
a64 1
  * GMR은 노드를 찾아서 메시지를 전송한다.
d75 14
a88 1
Socket은 여전히 붙어 있는데, 문제가 생길 경우가 있다. MQTT PING, 메시지 모니터링, 응답 지연 시간 확인 등 다양한 방법으로  문제를 진단해야 한다(다루어야 할 내용이 많다. 이건 별도의 문서로). 진단이 되면, LB를 조작해서 (깔끔하게) 문제가 되는 노드의 연결을 끊어 버린다. 물론 노드에 남아 있는 메시지는 에러 처리하던지, 외부 큐에 쌓아 두던지 조치를 취해야 할 것이다. 연결을 끊어 버리면, 클라이언트는 자연스럽게 다른 브로커로 분산될 거다. 그리고 개발자는 문제의 원인을 찾아서 해결하면 되겠다. 
d90 5
d96 1
@


1.3
log
@119.64.102.68;;yundream;;
@
text
@d72 5
@


1.2
log
@119.64.102.68;;yundream;;
@
text
@d61 11
@


1.1
log
@1.214.223.250;;yundream;;
@
text
@d38 1
a38 1
To가 개인인 경우에는 메시지를 한번만 전송하면 되지만, Group이라면 여러 번의 메시지 복사가 발생할 수 있다. 100명의 유저가 가입한 그룹이고, 마침 유저가 각 노드에 흩어져 있다면 최대 100번의 데이터복사가 발생할 수 있다. 그래서 HTTP 대신에 메시지 큐로 구성하기로 했다.  
d51 10
@
