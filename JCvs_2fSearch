#title JOINC 통한 검색엔진

 * 운영자 : [yundream]
'''목차'''

[[TableOfContents]]
== 관련문서 ==
[[subindex(JCvs/Search)]]

== 소개 ==
 <!> syntax 관련 부분에 대해서는 현재 통사론을 공부중이니, 이걸 토대로 재 작성되어야 할 것 같다. - 2008/10/28 
joinc는 지금 함수, (강좌)기사, wiki 등 3개의 커다른 컨텐츠로 구성되어 있다. 그런데 검색이 개별적으로 이루어지고 있어서 원하는 내용을 효과적으로 찾기 힘들도록 되어 있다. google를 검색엔진으로 활용할까 생각해 보았지만 사이트 특화되게 사용하기 위해서는 제약사항이 많음을 알게 되었다.   

위의 검색이 효과적으로 이루어지지 않는 다는 이유로 사이트 특화된 검색엔진을 만들기로 했다.  


== 작성 이유 ==
소개 부분에서 간단히 이유를 밝혔다. 현재 JOINC의 컨텐츠는 '''함수사전''', '''Docbook 컨텐츠''', '''wiki 컨텐츠''', '''게시판'''의 3개 부분으로 이루어져 있다. 또한 컨텐츠의 유지방법도 통일되어 있지 않다. 현재 함수사전과 Docbook 컨텐츠의 대부분은 mysql에 저장되어 있지만 wiki 컨텐츠의 경우 모두 로컬 파일시스템에 저장된다. 

이렇게 되어 있다보니 효율적인 검색이 매우 어렵도록 되어 있다. 검색엔진을 만드는 이유는 분리되어 있는 검색툴을 하나로 통합해서 보다 쉽고 효과적으로 원하는 컨텐츠를 찾도록 하는데 있다.  

=== 검색측면에서 본 사이트 구조의 변경 ===
적당한 검색 엔진을 작성하기 위해서 현재 사이트의 개략적인 구조를 먼저 기술하도록 한다. 
{{{#!plain
      +--------------------------------------------+
      | USER                                       | 
      +--------------------------------------------+
          |         |                  
          |         |                 
          |         |                
      +--------+    |       +----------------------+
      | JOINC  |    |       |  SQL SERVER          |        
      +--------+    |       |  +---------+ +-----+ |
      |        | <--------  |  | Article | | BBS | |
      |        |    |       |  +---------+ +-----+ |
      |        |    |       +----------------------+ 
      |        |    |       +----------------------+
      |        |    |       |  FILE SYSTEM         |
      |        |    |       |  +-------+ +------+  |
      |        |    +------ |  | JWIKI | | JCVS |  |
      |        |            |  +-------+ +------+  |
      +--------+            +----------------------+
}}}
위의 그림에서 보이듯이 사용자는 2가지 경로를 통해서 컨텐츠에 접근하게 된다. 내부적으로 구조가 복잡하거나 정리되어 있지 않다고 하더라도 최종적으로 사용자는 단일 통로를 유지하도록 만들어야 한다는 점에서 볼때 위의 구조에는 문제가 있다. 특히 검색에 있어서 문제가 발생한다. 검색의 문제외에도 인터페이스가 통일되어 있지 못하다는 문제가 있지만 일단 그건 논외로 하도록 하겠다.  
{{{#!plain
      +--------------------------------------------+
      | USER                                       | 
      +--------------------------------------------+
          |
      +--------------------------------------------+
      | Search Engine                              |
      +--------------------------------------------+ 
          |         |                 
          |         |                
      +--------+    |       +----------------------+
      | JOINC  |    |       |  SQL SERVER          |        
      +--------+    |       |  +---------+ +-----+ |
      |        | <--------  |  | Article | | BBS | |
      |        |    |       |  +---------+ +-----+ |
      |        |    |       +----------------------+ 
      |        |    |       +----------------------+
      |        |    |       |  FILE SYSTEM         |
      |        |    |       |  +-------+ +------+  |
      |        |    +------ |  | JWIKI | | JCVS |  |
      |        |            |  +-------+ +------+  |
      +--------+            +----------------------+
}}}
위에서 처럼 통함 검색 엔진을 둠으로써 사용자는 단일의 인터페이스를 가지고 원하는 컨텐츠를 얻어낼 수 있다. 

== 국어공부 -.-; ==
인덱스된 단어를 만들기 위해서 '''조사''', '''접속사'''등을 제거해야될 필요가 있다. 고로 국어에서의 단어 형태에 대한 기본적인 이해가 필요하다. 예를 들어 조사다음에 '''에서''','''그리고''', '''로서'''등이 올 수 없을 것이기 때문이다. 이러한 것을 고려하지 않을경우 '''수은'''같은 단어를 인덱스 하는데 문제가 발생할 것이다.  
'''수은'''단독으로 사용되는 경우는 어쩔수 없다고 하더라도, '''수은은''', '''수은에서''' 등은 충분히 분석해 낼 수 있어야 한다. 주격조사 두개가 사용될 수 없다와 '''에서'''앞에 주격조사가 사용될 수 없다는 걸 알고 있다면 충분히 분석해 낼 수 있다.   

여기에서 얻은 지식은 '''Parser'''모듈을 만드는데 도움이 될것이다. 


=== 국어의 특징 ===
 1. 첨가어(또는 교착어)이다.
 1. 주어+목적어+동사의 통사적 특징을 가지며 자리옮김이 비교적 자유롭다.
 1. 주어가 없는 문장이 흔히 쓰이며 조사 및 어미가 발달했다.   

=== 형태소와 단어 === 
==== 단위의 분석 원리 : '''대치'''와 '''결합''' ====
   1. 대치 : 어떤 말이 같은 성질을 가진 다른 말로 대치될 수 있으면 하나의 단위를 이루는 것으로 간주한다. 이때 대치될 수 있는 요소들 사이의 관계를 '''계열관계'''라고 한다.
  예) 철수가 | 학교에 | 갔다. |극장에|
  1. 결합 : 어떤 말이 자신의 앞이나 뒤에 다른말이 결합할 수 있으면 하나의 단위를 이루는 것으로 간주한다. 이때 결합될 수 있는 요소들 사이의 관계를 '''통합관계'''라고 한다.  
  예) 하늘이 ('''더욱''') 푸르다.
==== 어절과 구 ====
  1. 어절 : 문장을 구성하고 있는 도막도막의 마디를 '''어절'''이라고 하는데, 실제의 발화에서 어절의 앞이나 뒤에는 휴지(pause)를 둘 수 있다. 현행 맞춤법의 띄어쓰기는 어절을 단위로 하고 있다. 
  예) 철수가 | 동화를 || 읽었다. 
  2. 구 : 둘 또는 그 이상의 어절이 어울려서 큰 마디가 된 말을 '''구'''라고 한다.

==== 단어 ====
자립성이 있는 형태소를 '''단어'''라고 한다. '''자립성이 있다'''는 것은 앞이나 뒤에 휴지가 올 수 있음을 의미한다. 한편 국어의 조사는 비록 자립성은 없으나, 자립형태소에 결합하여 쓰이며, 앞에 다른말의 개입을 허용하는 등, 앞의 말과 쉽게 분리될 수 있는 특징을 지니고 있기 때문에 단어로 간주한다. 결국 국어 단어의 정립기준은 '''자립성'''과 '''분리성'''에 있다.

예) 철수가(철수-가) 집에(집-에) 갔다(가-았-다)
  
의존명사와 보조용언은 자립성이 결여되어 있으나, 일반적으로 자립형태소가 나타나는 환경에 쓰이며, 의미도 완전히 문법적이라고 할 수 없으므로 '''준자립어'''라고 부르며 단어로 간주한다.

예) 먹을 '''것'''이 많다. 한번 먹어 '''보아라'''. 

복수임을 나타내는 '''들'''은 자립성이 있는 형태소에 결합하여 쓰이지만, 특정 명사류에만 결합할 수 있다는 점에서 보편성이 결여되어 있으므로 단어의 자격이 없다. 

예) '''사람들이''' 많이 보였다. '''운영체제들의''' 특성을 설명하시오. 

=== 정리 ===
인덱스는 단어를 기준으로 한다. 그러므로 받아들인 문장을 '''어절'''로 구분하고 각 어절에서 의미있는 '''단어'''를 얻어오기 위해서 '''어미'''와 '''접사'''를 제거 시킨다. 

'''의미있는'''에 주목해야 한다. 먹을 '''것'''이 많다. 에서 '''먹'''과 '''것'''은 단어이지만 너무 짧고 일반적이기 때문에 이들 단어를 분류해 내지는 않는다. 특징적은 의미를 가지고 있는 단어에 한해서 분류해 낸다. 특징적인 의미를 구분해내는 기준은 단어의 길이로 한다. 영문은 3byte 한글은 4byte이상으로 한다. 

== 설계 ==
Down -> Top 방식으로 기술한다. 검색엔진을 위한 각 기능들의 일반사항을 기술하고 이를 토대로 전체적인 검색엔진 의 모양을 기술한다.  
=== 데이터 저장 일반 ===
URL 링크를 검색해서 웹페이지를 읽어 들이고 '''단어'''를 기준으로 인덱스를 작성한다. 이들 인덱스 정보들은 DB를 통해서 관리 되며, 실제 페이지는 로컬호스트에 일반 파일 형태로 존재하게 된다. 이들 파일은 페이지의 이름그대로 저장된다. 

인덱스 정보를 저장하기 위해서 2개의 테이블이 필요하다. 하나는 사이트(Site)이름을 저장하기 위한 용도로 사용되며 다른 하나는 페이지의 URL정보와 페이지가 저장된 위치 정보를 저장하기 위해서 사용된다. 

 * 사이트 이름을 저장하기 위한 테이블의 이름은 '''jsearch_url_index'''로 한다.
 * 페이지 정보를 저장하기 위한 테이블의 이름은 '''jsearch_page_index'''로 한다.   

인덱스 테이블이 만들어 졌다면 '''단어'''별 인덱스를 만들기 위한 테이블이 필요하다. 이 테이블에는 해당 단어를 포함한 jpi와 jui의 리스트를 가진다. 
 * 단어의 인덱스를 저장하는 테이블의 이름은 '''jsearch_word_index'''로 한다. 

이제 실제 내용을 포함한 파일이 저장되어야 하는데, 이들은 '''$pre_fix_dir/$jui/$jpi'''형태로 저장된다. 다음은 실제 저장 예이다. 
{{{#!plain
가정 
$pre_fix_dir = "/usr/jsearch"
$jui = 1 
$jpi = 2
$page_name = "/modules.php?name=test&file=myfile"

결과
저장된 파일이름 : "/usr/jsearch/1/2"
}}}
실제 페이지의 정확한 URL정보는 '''jui''', '''jpi''' 두개의 테이블에 있는 정보를 이용해서 얻어올 수 있다. 

'''단어'''인덱스는 읽어들인 페이지를 HTML파싱하고 이를 다시 '''[\n ]+'''를 기준으로 파싱해서 단어의 목록을 얻어내고 각 단어에 대한 jui와 jpi정보를 입력함으로써 만들어진다.    

==== 데이터 검색 일반 ====

=== 단어 인덱싱 방법 ===
==== 인덱싱 기준 ====
 * 단어 인덱싱은 '''plain/text'''와 '''plain/html'''페이지에 한해서 이루어진다. 
 * html문서의 경우 파싱을 거쳐서 테그는 모두 제거한다. '''<[a-zA-Z0-9\.\+\/-\_]+>'''와 '''<...\n\/>'''를 테그로 간주한다. 
 * 파싱은 '''[ \t]+'''을 기준으로 한다. 
 * 영문기준 3자 이상을 단어로 간주한다.  
 * [한글], [a-zA-Z0-9]외의 문자는 공백으로 처리해서 단어를 분리한다. 
   * 예) linux,unix,windows => linux unix windows 3개의 단어가 된다.
 * 조사와 ?? 는 제거한다. 제거할 조사의 목록은 아래와 같다.
 || 은		|| 는		|| 이		|| 가		||
 || 으로	|| 와		|| 부터 	|| 처럼		||
 || 까지	|| 에서		|| 과 		|| 로서		||
 || 로써	|| 이므로	|| 그러므로 || 그리고	||
 || 그래서	|| 고로		||			||			||
 * 영문과 한글이 조합되어 있을경우 대부분 영문+한글조사 이므로 한글을 제거한다.  
   * 예) IPC에서 => IPC, 제거방법은 단어의 첫글자를 보고 판단한다. 즉 처음 한바이트가 127 보다 작다면 영어로 판단하고 확장 ASCII가 최초로 발견되는 시점에서 다음의 모든 문자들은 삭제시킨다.  

== 설계도면 ==
여기 있는건 말그대로 도면이다. 실제 세부적인 구현에 있어서 자료형등은 (효율적인)자료형으로 변경될 것이다. 
=== 기본 골격 ===
이 프로그램은 다음과 같은 구성을 가지며, 크게 3개의 모듈로 이루어진다.
{{{#!plain
 +-------------------+      
 | WEB SERVER        |
 +-------------------+      
       | GET
   1.  |                    2.
  +----------+ page       +-------------+ page
  | Http Get | ---------> | PARSER      |-------------> FILE SYSTEM 
  +----------+            +=============+ 
       |  4.              | HTML parse  |
       | +-----------+    |             |
       +-| URL TABLE |<---|-----+       |
         +-----------+    |     |       |
                          |     |       |
                +---------| parsing     | 
                |         +-------------+
                | 
                | 단어/page index     3.                      
                |         +-------------+
                +-------->| DB Indexing |------------> DB SYSTEM 
                          +-------------+
}}}
 1. Webserver로 연결을 시도해서 요청한 페이지를 가져온다. 여러가지 방법이 있겠으나 우선은 '''GET'''방식만을 지원하도록 한다. 
 1. Http Get에서 가져온 page는 '''PARSER'''로 넘어간다. 여기에서는 HTML테그를 없애고 페이지로 부터 단어를 구분해낸다. 넘어온 page는 지정된 디스크 공간에 저장된다.  
 1. 단어의 목록은 '''DB Indexing'''으로 넘어오고 단어의 목록과 '''page index'''를 이용해서 DB에 저장한다.   
 1. PARSER는 HTML 파싱을 하면서 URL정보들은 별도로 URL TABLE에 저장한다. URL TABLE에 있는 정보들은 Http Get에서 페이지를 요청하기 위한 정보로 사용된다. 이 테이블은 '''stack'''형태로 유지된다. 단 중복되는 URL을 다시 검색할 필요는 없으므로 별도의 테이블을 유지해서 그동안 요청한 모든 URL의 페이지를 저장해 두어야 한다.   
=== Http Get ===
이것은 단일의 클래스로 이루어진다. '''WEB SERVER'''에 접근해서 페이지를 요청하며 읽어들인 페이지를 다른 모듈에서 요구할 때 되돌려준다. 

이 외에도 이 클래스는 "href"를 만났을 때 이를 분석해서 URL정보를 읽어와서 목록을 만들어야 한다. 기본적으로 링크기반으로 페이지를 수집하기 때문이다. 이러한 작업을 위해서 파싱을 해야 한다. 그러나 이 모듈은 별도의 파서를 포함하지 않으므로 '''PARSER'''로 부터 생성된 URL을 이용한다.  

이 모듈은 대략 다음과 같은 클래스로 만들어진다. 
{{{#!plain
class Http
{
    private:
        string host; 
        int  port;  
        int  sockfd;
        char *data;
		char *header;
        int  datalen;
        int  cdatasize;
        int  memsize;
        struct sockaddr_in serveraddr;
        int addrlen;

    public:
        Http();
        ~Http(); 
        int setHost(char *, int);
		struct header getHeader();
		int ContentType();
        char *Get(char *);
};
}}}
|| string 				|| host			|| 연결할 호스트 이름		||
|| int					|| port			|| 포트번호					||
|| int					|| sockfd		|| 연결 소켓				||
|| char *				|| data			|| 읽어들인 페이지			||
|| char * 				|| header		|| 페이지의 헤더 정보		||
|| int					|| datalen		|| 페이지의 크기			||
|| struct sockaddr_in	|| serveraddr	|| 서버주소					||
|| int					|| addrlen		|| 서버주소 구조체 길이		||

==== int Http::setHost(char *host, int port) ====
페이지를 요청할 서버 호스트 정보를 설정한다. 
 1. '''host''' : 호스트 이름
 1. '''port''' : 포트번호, 기본인자로 80이 사용된다. 

==== char *Http::Get(char *pageurl) ====
페이지를 요청한다. 페이지를 요청하기 전에 '''setHost'''정보를 이용해서 서버에 connect한다.  
 1. '''pageurl''' 요청할 페이지의 이름이다. 

pageurl을 이용해서 다음과 같이 접근을 시도한다.
{{{#!plain
GET /$pageurl HTTP/1.0\n\n
}}}

HTTP의 헤더는 다음과 같은 형식을 가진다.
{{{#!plain
HTTP/1.1 200 OK
Date: Thu, 25 Dec 2003 10:59:49 GMT
Server: Apache/1.3.28 (HPUX) PHP/4.3.22
X-Powered-By: PHP/4.3.22
Set-Cookie: lang=kor; expires=Fri, 24-Dec-2004 10:59:49 GMT
Set-Cookie: cycounter=cycounter; expires=Fri, 02-Jan-1970 00:00:00 GMT; path=/
Connection: close
Content-Type: text/html
}}}
요청한 데이터의 '''Content-Type'''를 알아야 하므로 헤더정보를 얻는 것은 매우 중요하다. 초기 버전에서는 단지 text/html문서만을 다룰 것이다. 다른 종류의 데이터들은 모두 버린다.    

==== int Http::ContentType() ====
가장 최근 문서의 ContentType을 되돌려준다. 현재는 '''text/html'''에 대한 것만 정의되어 있다. 만약 text/html이라면 1을 그렇지 않을경우 0을 되돌려준다. 

=== PARSER ===
Http::Get()을 이용해서 얻어온 문서를 파싱한다. 파싱하기 전에 Http::ContentType()를 이용해서 파싱가능한 데이터인지를 먼저 확인하고 파싱가능한 데이터라면 해당 파싱메서드를 실행 시킨다. 

파싱가능한 데이터는 원본을 파일시스템에 별도로 저장한다. 이 모듈은 다음과 같은 클래스로 구성된다. 
{{{#!plain
typedef struct _pageinfo
{
   char url[80];
   char page[1024];
} pageinfo;

class Parser
{
    private:
        set<string> mdata;
        set<string> pageUrl;
        pageinfo Pinfo;
        
    public:
        Parser();
        ~Parser();
        set<string> Rp(string);
        char * RemoveChosa(char *word);
        set<string> UrlInfo();
        pageinfo GetPinfo();
        int SplitUrl(string *);
};
}}}

|| 1 || set<string>		|| mdata	|| 파싱된 단어가 저장된다.			||
|| 2 || set<string>		|| pageUrl	|| 링크 정보가 저장된다.			||
|| 3 || pageinfo		|| Pinfo	|| 링크 정보구조체					||

 1. 인덱스된 단어가 저장된다. 
 1. 만들고자 하는 검색엔진은 링크를 이용해서 페이지 데이터를 읽어들인다. 그러므로 페이지를 파싱할때 특별히 url정보는 따로 자료구조에 저장해두어야 한다. 
 1. 저장되는 링크정보는 URL과 page로 나누어서 저장된다.  
{{{#!plain
예)
<a href=http://www.joinc.co.kr/modules/index.php&id=10>
pageinfo.url   = www.joinc.co.kr 
pageinfo.page  = modules/index.php&id=10 
}}}

==== Set<string> Parser::Rp(string data) ==== 
페이지를 읽어들여서 파싱한다. 파싱은 다음과 같은 프로세스로 진행된다.   
{{{#!plain
while(단어 길이 만큼)
{
   만약 "<"를 만난다면 ">"위치를 파악해서 테그인지 아닌지 판단한다. 
   영문/한글이 아닌 특수문자의 경우 모두 치환한다. 
   \t, \r, \n등을 기준으로 단어로 쪼갠다.
   쪼개진 단어의 길이가 4이상이면 '''mdata'''에 등록한다.  
}
}}}

==== int Parser::SplitUrl(string *tag) ==== 
'''href'''테그로 정의되는 링크데이터 문자열이 들어왔을 때 이를 URL과 페이지로 분리하고 부정확한 링크와 요청불가능한 링크는 제거한다. 예를 들어 '''mailto:''', '''ftp:'''등은 버린다. 

다음은 대략적인 코드이다.

리턴값은 주어진 링크데이터 '''tag'''의 분석결과다. 잘못된 링크들의 경우에는 -1을 그렇지 않을 경우 1을 리턴한다.
{{{#!plain
int Parser::SplitUrl(string *tag)
{
    // a href="http://www.join.co.kr"
    // a href='http://www.joinc.co.kr'
    // 처럼 구성되는 경우가 있다. 이때 ",'를 제거해준다.
    if(tag->find("\"") == 0)
        *tag = tag->substr(1, tag->find("\"", 1) -1);
    if(tag->find("'") == 0)
        *tag = tag->substr(1, tag->find("'", 1) -1);

    // anchor는 무시한다.
    if (tag->find("#") == 0)
    {
        return -1;
    }
    // ftp는 무시한다.
    if (strncmp(tag->c_str(), "ftp:", 4) == 0)
    {
        return -1;
    }
    // mail도 무시한다. 
    if (strncmp(tag->c_str(), "mailto:", 7) == 0)
    {
        return -1;
    }

    // 일반적으로 링크는 a href=http://www.joinc.co.kr.. 과같은 형식이지만
    // a href=www.joinc.co.kr..과 같이 구성될 수도 있다. 
    // 후자의 경우 잘못된 링크정보지만 꽤 많은 링크가 후자처럼 기술되므로 
    // 이를 처리하도록 한다. 
    if (strncmp(tag->c_str(), "http:", 5) == 0)
    {
        if(strncmp(tag->c_str()+5, "//", 2) == 0) 
            sscanf(tag->c_str(), "%[^:]%[:]%[^a-zA-Z0-9]%[^/]%s", 
                 null,null, null, Pinfo.url, Pinfo.page);
        else
            sscanf(tag->c_str(), "%[^:]%[:]%[^/]%s", 
                 null, null, Pinfo.url, Pinfo.page);
        return 0;
    }
    return 1;
}
}}}


==== char *Parser::RemoveChosa(char *word) ====
주어진 단어 word에서 조사를 제거한다. 조사의 목록은 (변경가능하다)다음과 같이 주어질 것이다.  
{{{#!plain
char *chosa[] ={"있습니다","그러므로","왜냐하면",
                "그래서", "그리고",  "왜냐면", "입니다", "이므로",
                "에서", "처럼", "보다", "로서", "로써", "까지",
                "한다", "하고", "에게","하기", "하는", "부터",
                "와", "을","를", "은","는", "이", "가", "의", "등", "에",
                "한",
                NULL};
}}}
위 배열에는 조사 뿐만 아니라 접속사등 제거해도 되는 모든 단어 목록이 들어갈 수 있다. 단어를 등록할 때는 길이가 가장 긴것부터 등록하도록 한다. 다음은 조사를 제거하기 위한 대략적인 코드다. 
{{{#!plain
char *Parser::RemoveChosa(char *word)
{
    char *cl;
    int i = 0, j; 
    vector<int> llist;
    while((cl = *(chosa +i)) != NULL)
    {
        for (j = 0; j < llist.size(); j++)
        {
            if (llist[j] == i)
                goto La;
        }
        if(strlen(word) >= strlen(cl)) 
        {       
            if (strcmp(cl, word+(strlen(word)-strlen(cl))) == 0)
            {   
                cout << "조사 발견" << word <<endl;
                word[strlen(word)-strlen(cl)] = 0x00;
                llist.push_back(i);
                i = 0;
            }
        }
La:
        i++;
    }
    cout << "리턴값은 " << word << endl;
    if (strlen(word) == 0)
        word = NULL;
    return word;
}
}}}
위의 코드를 보면 조사로 판단되는 문자가 더이상 발생하지 않을 때까지 chosa배열을 순환하고 있음을 알 수 있다. 예를 들어'''학교에서''', '''학교에서는'''과 같은 단어가 있다면 한번만 순환하게 될경우 전자의 경우 '''학교'''로 나오지만 후자의 경우단지 '''는'''만 제거 되기 때문에 '''학교에서'''가 단어로 리턴되는 것과 같은 문제가 발생할 수 있기 때문이다. 그러므로 더이상 조사로 판단되는게 발견되지 않을 때까지 순환할 필요가 있다.     

이미 검사한 조사는 다시 검사할 필요가 없으므로 llist에 넣어서 따로 관리한다.  
 * 위의 경우 문제가 있다. '''수은'''을 예로 들어보면, '''수은에서'''라는 단어가 있다고 할 때 위의 방식으로만 한다면 '''에서'''와 '''은'''이 모두 제거되어 버리는 문제가 발생하게 된다. 이 문제는 당분간 무시한다. 

=== DB Indexing ===
이 모듈은 Parser::Rp로 부터 Parser모듈에서 만들어진 단어목록과 URL과 페이지 경로등을 이용해서 DB와 FILE시스템에 저장한다.   

다음은 대략적인 프로세스다.
{{{#!plain

int main()
{
  sql 서버에 연결
  {
    jsearch_url_index 테이블에 URL이 등록되어 있는지 확인한다. 
    만약 등록되어 있지 않다면 
    {
      DB에 저장 {jui | url} 
    }
   
    jsearch_page_index 테이블내에 페이지 경로가 등록되어 있는지 확인한다.
    만약 등록되어 있지 않다면 
    {
      DB에 저장 {jui | upi | page 경로} 
    }

    jui와 jpi를 얻어온다. 
  }
  만약 페이지의 contenttype이 text/html 혹은 text/plain 이라면 
  {
     파일 시스템에 저장한다.  
     저장 경로는 prefix_dir/jui/jpi

     DB에 저장한다.{단어 | jui | jpi | contenttype | time} 
  }
}
}}}

=== DB 테이블 정보 ===
여기에서는 단어 인덱스와 URL, 페이지 경로의 데이터를 저장하기 위한 테이블에 대해서 설명한다. 프로토타입이며 실제 코딩에서는 필요에 따라 변경될 수 있다.  

==== jsearch_url_index ====
이 테이블에는 URL들에 대한 정보가 들어간다. 현재 단계에서는 단지 joinc.co.kr의 컨텐츠만을 인덱스하므로 필요 없는 테이블이라고 볼수도 있지만 확장을 위해서는 반드시 필요하다. 
 ||	Field	|| Type			|| NULL		|| Key	|| Default	|| Extra			||
 || ID		|| int			||			|| PRI	||			|| auto_increment	||
 || URL		|| varchar(80)	||			||		||			||					||

 1. '''ID''' : URL에 대한 유일한 번호
 1. '''URL''' : 실제 URL 이름

==== jsearch_page_index ====
이 테이블에는 페이지 경로가 들어간다 페이지 경로는 URL에 대해서 유일하게 되므로 jsearch_url_index테이블을 참조한다.  
 ||	Field	|| Type			|| NULL		|| Key	|| Default	|| Extra			||
 || ID		|| int			||			|| PRI	||			|| auto_increment	||
 || UID		|| int			||			||		||			||					||
 || Page	|| varchar(256)	||			|| PRI	||			||					||
 1. '''ID''' : 페이지가 존재하는 URL에대한 ID 
 1. '''UID''' : 페이지에 대한 유일한 번호 
 1. '''Page''' : 페이지의 경로를 포함한 실제 이름, 요청 문자열까지 포함한다.

==== jsearch_word_index ====

== 잡담들 ==
 1. [http://www.netsw.org/type/html html 파서들]
 1. http://database.sarang.net/?inc=read&aid=5125&criteria=pgsql&subcrit=qna&id=&limit=20&keyword=&page=
